{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sIdWSe-WiWX",
        "outputId": "52508c26-7738-45ab-912e-155dec98557e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=a5cd6392cd9cf2fd99987db3169c461dd1fa0a93783c0df85a76723254d6888e\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bCWLHu9u7jvY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "from torch_geometric.utils import (add_self_loops, negative_sampling,\n",
        "                                   remove_self_loops)\n",
        "EPS = 1e-15\n",
        "MAX_LOGSTD = 10\n",
        "\n",
        "class InnerProductDecoder(torch.nn.Module):\n",
        "    r\"\"\"The inner product decoder from the `\"Variational Graph Auto-Encoders\"\n",
        "    <https://arxiv.org/abs/1611.07308>`_ paper\n",
        "\n",
        "    .. math::\n",
        "        \\sigma(\\mathbf{Z}\\mathbf{Z}^{\\top})\n",
        "\n",
        "    where :math:`\\mathbf{Z} \\in \\mathbb{R}^{N \\times d}` denotes the latent\n",
        "    space produced by the encoder.\"\"\"\n",
        "    def forward(self, z, edge_index, sigmoid=True):\n",
        "        r\"\"\"Decodes the latent variables :obj:`z` into edge probabilities for\n",
        "        the given node-pairs :obj:`edge_index`.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
        "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
        "                the logistic sigmoid function to the output.\n",
        "                (default: :obj:`True`)\n",
        "        \"\"\"\n",
        "        value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
        "        return torch.sigmoid(value) if sigmoid else value\n",
        "\n",
        "\n",
        "    def forward_all(self, z, sigmoid=True):\n",
        "        r\"\"\"Decodes the latent variables :obj:`z` into a probabilistic dense\n",
        "        adjacency matrix.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
        "            sigmoid (bool, optional): If set to :obj:`False`, does not apply\n",
        "                the logistic sigmoid function to the output.\n",
        "                (default: :obj:`True`)\n",
        "        \"\"\"\n",
        "        adj = torch.matmul(z, z.t())\n",
        "        return torch.sigmoid(adj) if sigmoid else adj\n",
        "\n",
        "\n",
        "\n",
        "class GAE(torch.nn.Module):\n",
        "    r\"\"\"The Graph Auto-Encoder model from the\n",
        "    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\n",
        "    paper based on user-defined encoder and decoder models.\n",
        "\n",
        "    Args:\n",
        "        encoder (Module): The encoder module.\n",
        "        decoder (Module, optional): The decoder module. If set to :obj:`None`,\n",
        "            will default to the\n",
        "            :class:`torch_geometric.nn.models.InnerProductDecoder`.\n",
        "            (default: :obj:`None`)\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder=None):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = InnerProductDecoder() if decoder is None else decoder\n",
        "    \n",
        "\n",
        "    def encode(self, *args, **kwargs):\n",
        "        r\"\"\"Runs the encoder and computes node-wise latent variables.\"\"\"\n",
        "        return self.encoder(*args, **kwargs)\n",
        "\n",
        "\n",
        "    def decode(self, *args, **kwargs):\n",
        "        r\"\"\"Runs the decoder and computes edge probabilities.\"\"\"\n",
        "        return self.decoder(*args, **kwargs)\n",
        "\n",
        "\n",
        "    def recon_loss(self, z, pos_edge_index, neg_edge_index=None):\n",
        "        r\"\"\"Given latent variables :obj:`z`, computes the binary cross\n",
        "        entropy loss for positive edges :obj:`pos_edge_index` and negative\n",
        "        sampled edges.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
        "            pos_edge_index (LongTensor): The positive edges to train against.\n",
        "            neg_edge_index (LongTensor, optional): The negative edges to train\n",
        "                against. If not given, uses negative sampling to calculate\n",
        "                negative edges. (default: :obj:`None`)\n",
        "        \"\"\"\n",
        "\n",
        "        pos_loss = -torch.log(\n",
        "            self.decode(z)[pos_edge_index[0],pos_edge_index[1]] + EPS).mean()\n",
        "\n",
        "        # Do not include self-loops in negative samples\n",
        "        pos_edge_index, _ = remove_self_loops(pos_edge_index)\n",
        "        pos_edge_index, _ = add_self_loops(pos_edge_index)\n",
        "        if neg_edge_index is None:\n",
        "            neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\n",
        "        neg_loss = -torch.log(1 -\n",
        "                              self.decode(z)[neg_edge_index[0],neg_edge_index[1]] +\n",
        "                              EPS).mean()\n",
        "\n",
        "        return pos_loss + neg_loss\n",
        "\n",
        "    #define test function based on decoder\n",
        "    def test(self, z, pos_edge_index, neg_edge_index):\n",
        "        r\"\"\"Given latent variables :obj:`z`, positive edges\n",
        "        :obj:`pos_edge_index` and negative edges :obj:`neg_edge_index`,\n",
        "        computes area under the ROC curve (AUC) and average precision (AP)\n",
        "        scores.\n",
        "\n",
        "        Args:\n",
        "            z (Tensor): The latent space :math:`\\mathbf{Z}`.\n",
        "            pos_edge_index (LongTensor): The positive edges to evaluate\n",
        "                against.\n",
        "            neg_edge_index (LongTensor): The negative edges to evaluate\n",
        "                against.\n",
        "        \"\"\"\n",
        "        from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "\n",
        "        pos_y = z.new_ones(pos_edge_index.size(1))\n",
        "        neg_y = z.new_zeros(neg_edge_index.size(1))\n",
        "        y = torch.cat([pos_y, neg_y], dim=0)\n",
        "\n",
        "        pred_test_neg=model.decode(z)[data.test_neg_edge_index[0],data.test_neg_edge_index[1]]\n",
        "        pred_test_pos=model.decode(z)[data.test_pos_edge_index[0],data.test_pos_edge_index[1]]\n",
        "        pred=torch.cat([pred_test_pos,pred_test_neg],dim=0)\n",
        "\n",
        "        y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
        "  \n",
        "        return roc_auc_score(y, pred), average_precision_score(y, pred)\n",
        "\n",
        "\n",
        "\n",
        "class VGAE(GAE):\n",
        "    r\"\"\"The Variational Graph Auto-Encoder model from the\n",
        "    `\"Variational Graph Auto-Encoders\" <https://arxiv.org/abs/1611.07308>`_\n",
        "    paper.\n",
        "\n",
        "    Args:\n",
        "        encoder (Module): The encoder module to compute :math:`\\mu` and\n",
        "            :math:`\\log\\sigma^2`.\n",
        "        decoder (Module, optional): The decoder module. If set to :obj:`None`,\n",
        "            will default to the\n",
        "            :class:`torch_geometric.nn.models.InnerProductDecoder`.\n",
        "            (default: :obj:`None`)\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder=None):\n",
        "        super().__init__(encoder, decoder)\n",
        "\n",
        "    def reparametrize(self, mu, logstd):\n",
        "        if self.training:\n",
        "            return mu + torch.randn_like(logstd) * torch.exp(logstd)\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "\n",
        "    def encode(self, *args, **kwargs):\n",
        "        \"\"\"\"\"\"\n",
        "        self.__mu__, self.__logstd__ = self.encoder(*args, **kwargs)\n",
        "        self.__logstd__ = self.__logstd__.clamp(max=MAX_LOGSTD)\n",
        "        z = self.reparametrize(self.__mu__, self.__logstd__)\n",
        "        return z\n",
        "\n",
        "\n",
        "    def kl_loss(self, mu=None, logstd=None):\n",
        "        r\"\"\"Computes the KL loss, either for the passed arguments :obj:`mu`\n",
        "        and :obj:`logstd`, or based on latent variables from last encoding.\n",
        "\n",
        "        Args:\n",
        "            mu (Tensor, optional): The latent space for :math:`\\mu`. If set to\n",
        "                :obj:`None`, uses the last computation of :math:`mu`.\n",
        "                (default: :obj:`None`)\n",
        "            logstd (Tensor, optional): The latent space for\n",
        "                :math:`\\log\\sigma`.  If set to :obj:`None`, uses the last\n",
        "                computation of :math:`\\log\\sigma^2`.(default: :obj:`None`)\n",
        "        \"\"\"\n",
        "        mu = self.__mu__ if mu is None else mu\n",
        "        logstd = self.__logstd__ if logstd is None else logstd.clamp(\n",
        "            max=MAX_LOGSTD)\n",
        "        return -0.5 * torch.mean(\n",
        "            torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kFUJ3tGtWlo7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(\"\\..\", \"CiteSeer\", transform=T.NormalizeFeatures())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xov721x4s1BF",
        "outputId": "d943a724-040e-48dd-af5b-18b6dabf2700"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML0O3ay5WmAz",
        "outputId": "dbd890d5-f601-4cd5-ca44-ca64de40cb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "data = dataset[0]\n",
        "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
        "data = train_test_split_edges(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1YLu_8K5lfL",
        "outputId": "a42370f2-86f2-4ee7-938f-3f6c053ae206"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[3327, 3703], val_pos_edge_index=[2, 227], test_pos_edge_index=[2, 455], train_pos_edge_index=[2, 7740], train_neg_adj_mask=[3327, 3327], val_neg_edge_index=[2, 227], test_neg_edge_index=[2, 455])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BBugwKP9zzU7"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import to_dense_adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-8hIP7Z1YMKF"
      },
      "outputs": [],
      "source": [
        "class VariationalGCNEncoder(torch.nn.Module):\n",
        "    def __init__(self,embedding_size,in_channels, out_channels):\n",
        "        super(VariationalGCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * embedding_size, cached=True) # cached only for transductive learning\n",
        "        self.conv_mu = GCNConv(2 * embedding_size, out_channels, cached=True)\n",
        "        self.conv_logstd = GCNConv(2 * embedding_size, out_channels, cached=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        hidden = self.conv1(x, edge_index).relu()\n",
        "\n",
        "        z_mu=self.conv_mu(hidden, edge_index)\n",
        "        z_lsgms=self.conv_logstd(hidden, edge_index)\n",
        "        return z_mu,z_lsgms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mQLkbZ2kYeSZ"
      },
      "outputs": [],
      "source": [
        "# definition of terms\n",
        "# h: hidden state of LSTM\n",
        "# y: edge prediction, model output\n",
        "# n: noise for generator\n",
        "# l: whether an output is real or not, binary\n",
        "import torch.nn.functional as F\n",
        "# plain LSTM model\n",
        "class RNN_plain(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_layers,output_size):\n",
        "        super(RNN_plain, self).__init__()\n",
        "        torch.manual_seed(1)\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        #self.batch_size=batch_size\n",
        "\n",
        "\n",
        "        #self.input = nn.Linear(input_size, embedding_size)\n",
        "        #self.linear_input = nn.Linear(input_size, embedding_size)\n",
        "\n",
        "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers)\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "                nn.Linear(hidden_size, output_size),\n",
        "                nn.ReLU()\n",
        "                #nn.Linear(4*hidden_size, 8*hidden_size),\n",
        "                #nn.ReLU(),\n",
        "                #nn.Linear(8*hidden_size,output_size)\n",
        "           )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "       \n",
        "        #self.tanh = nn.Tanh()\n",
        "        #self.dropout= nn.Dropout(p=0.3)\n",
        "        # initialize\n",
        "\n",
        "        self.hidden = None\n",
        "\n",
        "        for name, param in self.rnn.named_parameters():\n",
        "            if 'bias' in name:\n",
        "                nn.init.constant(param, 0.25)\n",
        "            elif 'weight' in name:\n",
        "                nn.init.xavier_uniform(param,gain=nn.init.calculate_gain('sigmoid'))\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data = init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1,1, self.hidden_size).cuda()\n",
        "\n",
        "    def forward(self, z):\n",
        "        #input = self.relu(self.input(z))\n",
        "        #input = F.dropout(input, p=0.2)\n",
        "        #input = self.dropout(input)\n",
        "        #input = self.relu(input)\n",
        "        #linear shape torch.Size([3327, 512])\n",
        "        h0 = self.init_hidden()\n",
        "        #print(h0.shape)\n",
        "        input=z.unsqueeze(1)\n",
        "        #print(input.shape)\n",
        "        output_raw, h_out = self.rnn(input, h0)\n",
        "        #print(output_raw.shape)\n",
        "        #output_raw = self.tanh(output_raw)\n",
        "        #lstm shape torch.Size([1, 3327, 1024]\n",
        "        output_raw = self.output(output_raw.view(-1, output_raw.size(2)))\n",
        "        #output_raw = F.dropout(output_raw, p=0.2)\n",
        "\n",
        "        # return hidden state at each time step\n",
        "        #output_raw=output_raw.view(-1, output_raw.size(2))\n",
        "        #linear shape torch.Size([3327, 3327])\n",
        "        return torch.sigmoid(output_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iPnoTi1ZXr9t"
      },
      "outputs": [],
      "source": [
        "#out_channels = 2\n",
        "num_features = dataset.num_features\n",
        "epochs = 300\n",
        "embedding_size=64\n",
        "hidden_size=256\n",
        "num_layers=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SxB24-MBP3M",
        "outputId": "15cda81b-2171-48dd-8767-ce72b3af96fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = data.x.to(device)\n",
        "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
        "model=VGAE(VariationalGCNEncoder(embedding_size=embedding_size,in_channels=num_features, out_channels=embedding_size),\n",
        "           decoder=RNN_plain(input_size=embedding_size, embedding_size=embedding_size,hidden_size=hidden_size, num_layers=num_layers,output_size=3327))\n",
        "model = model.to(device)\n",
        "#test_pos_edge_index=data.test_pos_edge_index.to(device)\n",
        "#test_neg_edge_index=data.test_neg_edge_index.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zajkRuwxZmeq",
        "outputId": "191ae1ad-abe1-4f95-ed20-5497a19b11cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGAE(\n",
              "  (encoder): VariationalGCNEncoder(\n",
              "    (conv1): GCNConv(3703, 128)\n",
              "    (conv_mu): GCNConv(128, 64)\n",
              "    (conv_logstd): GCNConv(128, 64)\n",
              "  )\n",
              "  (decoder): LSTM_plain(\n",
              "    (rnn): RNN(64, 256)\n",
              "    (output): Sequential(\n",
              "      (0): Linear(in_features=256, out_features=3327, bias=True)\n",
              "      (1): ReLU()\n",
              "    )\n",
              "    (relu): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.x.size(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgZJlRUb6nj_",
        "outputId": "ecaa526d-a94d-497f-b1ab-af66d74a4df5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3327"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['x'].float().size(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQJRo2d36NKx",
        "outputId": "5a36098d-d241-470d-fea0-a7d75104d623"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3327"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ale6gavCYrw_"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    #model.decoder.hidden=model.decoder.init_hidden(batch_size=data['x'].float())\n",
        "    z = model.encode(x.to(device), train_pos_edge_index.to(device))\n",
        "    loss = model.recon_loss(z, train_pos_edge_index)\n",
        "\n",
        "    loss =  (1 / data.num_nodes) * model.kl_loss()  # new line\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(\"loss:\",float(loss))\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_pos_edge_index)\n",
        "    return model.test(z,pos_edge_index, neg_edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r1ypGtEpEKTc"
      },
      "outputs": [],
      "source": [
        "#from torch.utils.tensorboard import SummaryWriter\n",
        "#writer = SummaryWriter('runs/VGAE+LSTM_experiment_'+'2d_100_epochs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu3oBXAlDgPJ",
        "outputId": "9b328fa6-1ccf-44b6-c218-c4e52397bd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.4662460046110937e-07\n",
            "Epoch: 001,AUC: 0.4747, AP: 0.4816\n",
            "loss: 3.546807647580863e-06\n",
            "Epoch: 002,AUC: 0.4748, AP: 0.4817\n",
            "loss: 3.418600442728348e-07\n",
            "Epoch: 003,AUC: 0.4755, AP: 0.4821\n",
            "loss: 4.6456875679723453e-07\n",
            "Epoch: 004,AUC: 0.4760, AP: 0.4826\n",
            "loss: 1.3532331877286197e-06\n",
            "Epoch: 005,AUC: 0.4760, AP: 0.4826\n",
            "loss: 1.2235166195750935e-06\n",
            "Epoch: 006,AUC: 0.4765, AP: 0.4822\n",
            "loss: 5.785431085314485e-07\n",
            "Epoch: 007,AUC: 0.4760, AP: 0.4825\n",
            "loss: 1.9181159416348237e-07\n",
            "Epoch: 008,AUC: 0.4753, AP: 0.4824\n",
            "loss: 2.658650259945716e-07\n",
            "Epoch: 009,AUC: 0.4753, AP: 0.4823\n",
            "loss: 5.043194164500164e-07\n",
            "Epoch: 010,AUC: 0.4752, AP: 0.4822\n",
            "loss: 5.829413680658035e-07\n",
            "Epoch: 011,AUC: 0.4747, AP: 0.4823\n",
            "loss: 4.5270510895534244e-07\n",
            "Epoch: 012,AUC: 0.4747, AP: 0.4820\n",
            "loss: 2.6671600039662735e-07\n",
            "Epoch: 013,AUC: 0.4753, AP: 0.4823\n",
            "loss: 1.685663022499284e-07\n",
            "Epoch: 014,AUC: 0.4753, AP: 0.4820\n",
            "loss: 1.8214620922663016e-07\n",
            "Epoch: 015,AUC: 0.4754, AP: 0.4822\n",
            "loss: 2.3769300128151372e-07\n",
            "Epoch: 016,AUC: 0.4755, AP: 0.4825\n",
            "loss: 2.580628688519937e-07\n",
            "Epoch: 017,AUC: 0.4749, AP: 0.4826\n",
            "loss: 2.2199908755737852e-07\n",
            "Epoch: 018,AUC: 0.4750, AP: 0.4823\n",
            "loss: 1.6130159963267943e-07\n",
            "Epoch: 019,AUC: 0.4754, AP: 0.4825\n",
            "loss: 1.1671904331933547e-07\n",
            "Epoch: 020,AUC: 0.4753, AP: 0.4820\n",
            "loss: 1.0613998568942407e-07\n",
            "Epoch: 021,AUC: 0.4752, AP: 0.4823\n",
            "loss: 1.1840310776278784e-07\n",
            "Epoch: 022,AUC: 0.4752, AP: 0.4819\n",
            "loss: 1.2786244951712433e-07\n",
            "Epoch: 023,AUC: 0.4747, AP: 0.4820\n",
            "loss: 1.1791042453523914e-07\n",
            "Epoch: 024,AUC: 0.4748, AP: 0.4823\n",
            "loss: 9.16553588581337e-08\n",
            "Epoch: 025,AUC: 0.4752, AP: 0.4823\n",
            "loss: 6.745160874288558e-08\n",
            "Epoch: 026,AUC: 0.4753, AP: 0.4824\n",
            "loss: 5.9461321200160455e-08\n",
            "Epoch: 027,AUC: 0.4749, AP: 0.4825\n",
            "loss: 6.536446051086386e-08\n",
            "Epoch: 028,AUC: 0.4750, AP: 0.4826\n",
            "loss: 7.037181859459452e-08\n",
            "Epoch: 029,AUC: 0.4750, AP: 0.4823\n",
            "loss: 6.350125403287166e-08\n",
            "Epoch: 030,AUC: 0.4750, AP: 0.4823\n",
            "loss: 4.796857311362146e-08\n",
            "Epoch: 031,AUC: 0.4754, AP: 0.4824\n",
            "loss: 3.626083611152353e-08\n",
            "Epoch: 032,AUC: 0.4754, AP: 0.4824\n",
            "loss: 3.523965830254383e-08\n",
            "Epoch: 033,AUC: 0.4753, AP: 0.4823\n",
            "loss: 3.9136264007311183e-08\n",
            "Epoch: 034,AUC: 0.4748, AP: 0.4820\n",
            "loss: 3.9216882186110524e-08\n",
            "Epoch: 035,AUC: 0.4748, AP: 0.4820\n",
            "loss: 3.236423040675618e-08\n",
            "Epoch: 036,AUC: 0.4748, AP: 0.4824\n",
            "loss: 2.423062284151456e-08\n",
            "Epoch: 037,AUC: 0.4748, AP: 0.4821\n",
            "loss: 2.0665451572199345e-08\n",
            "Epoch: 038,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.1946405581729778e-08\n",
            "Epoch: 039,AUC: 0.4749, AP: 0.4826\n",
            "loss: 2.312882330102184e-08\n",
            "Epoch: 040,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.0728155192273334e-08\n",
            "Epoch: 041,AUC: 0.4749, AP: 0.4826\n",
            "loss: 1.5962651644940706e-08\n",
            "Epoch: 042,AUC: 0.4750, AP: 0.4825\n",
            "loss: 1.2702042084811183e-08\n",
            "Epoch: 043,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.272891481107763e-08\n",
            "Epoch: 044,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.3445532687228479e-08\n",
            "Epoch: 045,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.27468302579814e-08\n",
            "Epoch: 046,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.011326222766229e-08\n",
            "Epoch: 047,AUC: 0.4749, AP: 0.4821\n",
            "loss: 8.474000168234852e-09\n",
            "Epoch: 048,AUC: 0.4749, AP: 0.4825\n",
            "loss: 7.72155139827646e-09\n",
            "Epoch: 049,AUC: 0.4749, AP: 0.4821\n",
            "loss: 7.766340459625098e-09\n",
            "Epoch: 050,AUC: 0.4749, AP: 0.4822\n",
            "loss: 7.506566035431206e-09\n",
            "Epoch: 051,AUC: 0.4749, AP: 0.4822\n",
            "loss: 6.646625472228607e-09\n",
            "Epoch: 052,AUC: 0.4749, AP: 0.4822\n",
            "loss: 5.45524914130624e-09\n",
            "Epoch: 053,AUC: 0.4749, AP: 0.4825\n",
            "loss: 4.523646790488556e-09\n",
            "Epoch: 054,AUC: 0.4749, AP: 0.4822\n",
            "loss: 4.649054918814954e-09\n",
            "Epoch: 055,AUC: 0.4749, AP: 0.4822\n",
            "loss: 4.6221817484592975e-09\n",
            "Epoch: 056,AUC: 0.4749, AP: 0.4825\n",
            "loss: 4.281788701376854e-09\n",
            "Epoch: 057,AUC: 0.4749, AP: 0.4821\n",
            "loss: 3.3591436299218458e-09\n",
            "Epoch: 058,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.9023001779648894e-09\n",
            "Epoch: 059,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.8575115607054613e-09\n",
            "Epoch: 060,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.8754270076092325e-09\n",
            "Epoch: 061,AUC: 0.4748, AP: 0.4825\n",
            "loss: 2.5977378026453835e-09\n",
            "Epoch: 062,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.9975707754582572e-09\n",
            "Epoch: 063,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.069232563073342e-09\n",
            "Epoch: 064,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.8005009705390762e-09\n",
            "Epoch: 065,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.89007809403563e-09\n",
            "Epoch: 066,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.5765581062865408e-09\n",
            "Epoch: 067,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.1555454371148244e-09\n",
            "Epoch: 068,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.0211796963588426e-09\n",
            "Epoch: 069,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.012221972906957e-09\n",
            "Epoch: 070,AUC: 0.4749, AP: 0.4824\n",
            "loss: 9.584756321956434e-10\n",
            "Epoch: 071,AUC: 0.4749, AP: 0.4825\n",
            "loss: 8.957716790547465e-10\n",
            "Epoch: 072,AUC: 0.4748, AP: 0.4825\n",
            "loss: 6.180824185797462e-10\n",
            "Epoch: 073,AUC: 0.4749, AP: 0.4820\n",
            "loss: 6.807864272317943e-10\n",
            "Epoch: 074,AUC: 0.4749, AP: 0.4821\n",
            "loss: 6.359978654835174e-10\n",
            "Epoch: 075,AUC: 0.4749, AP: 0.4821\n",
            "loss: 5.553784099276982e-10\n",
            "Epoch: 076,AUC: 0.4749, AP: 0.4826\n",
            "loss: 3.67266383971554e-10\n",
            "Epoch: 077,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.2247779446770153e-10\n",
            "Epoch: 078,AUC: 0.4749, AP: 0.4822\n",
            "loss: 3.762241074234396e-10\n",
            "Epoch: 079,AUC: 0.4749, AP: 0.4822\n",
            "loss: 3.941395265716352e-10\n",
            "Epoch: 080,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.4935093706778275e-10\n",
            "Epoch: 081,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.866469284157347e-10\n",
            "Epoch: 082,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.3290064321557225e-10\n",
            "Epoch: 083,AUC: 0.4749, AP: 0.4821\n",
            "loss: 2.776892049638491e-10\n",
            "Epoch: 084,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 085,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.956046518676203e-10\n",
            "Epoch: 086,AUC: 0.4749, AP: 0.4819\n",
            "loss: 2.866469284157347e-10\n",
            "Epoch: 087,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.045623753195059e-10\n",
            "Epoch: 088,AUC: 0.4749, AP: 0.4822\n",
            "loss: 3.4039321361589714e-10\n",
            "Epoch: 089,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.4039321361589714e-10\n",
            "Epoch: 090,AUC: 0.4749, AP: 0.4821\n",
            "loss: 3.135200710158159e-10\n",
            "Epoch: 091,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 092,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.5977378581565347e-10\n",
            "Epoch: 093,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 094,AUC: 0.4749, AP: 0.4822\n",
            "loss: 3.67266383971554e-10\n",
            "Epoch: 095,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.135200710158159e-10\n",
            "Epoch: 096,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.4332346420786735e-10\n",
            "Epoch: 097,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.776892049638491e-10\n",
            "Epoch: 098,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.7915433025983418e-10\n",
            "Epoch: 099,AUC: 0.4749, AP: 0.4822\n",
            "loss: 4.210126691717164e-10\n",
            "Epoch: 100,AUC: 0.4749, AP: 0.4821\n",
            "loss: -4.4788582564958546e-11\n",
            "Epoch: 101,AUC: 0.4749, AP: 0.4820\n",
            "loss: 2.1498519631180102e-10\n",
            "Epoch: 102,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.3436574075598173e-10\n",
            "Epoch: 103,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 104,AUC: 0.4749, AP: 0.4823\n",
            "loss: 2.776892049638491e-10\n",
            "Epoch: 105,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.1645032160778612e-10\n",
            "Epoch: 106,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.4332346420786735e-10\n",
            "Epoch: 107,AUC: 0.4749, AP: 0.4824\n",
            "loss: 2.1498519631180102e-10\n",
            "Epoch: 108,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.060274728599154e-10\n",
            "Epoch: 109,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.7019660680794857e-10\n",
            "Epoch: 110,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.7019660680794857e-10\n",
            "Epoch: 111,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.7915433025983418e-10\n",
            "Epoch: 112,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.135200710158159e-10\n",
            "Epoch: 113,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.045623753195059e-10\n",
            "Epoch: 114,AUC: 0.4749, AP: 0.4822\n",
            "loss: 3.5830866051966836e-10\n",
            "Epoch: 115,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.5081606236376786e-10\n",
            "Epoch: 116,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.3290064321557225e-10\n",
            "Epoch: 117,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.3290064321557225e-10\n",
            "Epoch: 118,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.1498519631180102e-10\n",
            "Epoch: 119,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.5228118765975296e-10\n",
            "Epoch: 120,AUC: 0.4749, AP: 0.4825\n",
            "loss: 6.270401559094196e-11\n",
            "Epoch: 121,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.7019660680794857e-10\n",
            "Epoch: 122,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.4332346420786735e-10\n",
            "Epoch: 123,AUC: 0.4749, AP: 0.4823\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 124,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.3143551791958714e-10\n",
            "Epoch: 125,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 126,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.5081606236376786e-10\n",
            "Epoch: 127,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.5977378581565347e-10\n",
            "Epoch: 128,AUC: 0.4749, AP: 0.4822\n",
            "loss: 9.85348816429088e-11\n",
            "Epoch: 129,AUC: 0.4749, AP: 0.4824\n",
            "loss: 2.3290064321557225e-10\n",
            "Epoch: 130,AUC: 0.4749, AP: 0.4820\n",
            "loss: 2.4185833891188224e-10\n",
            "Epoch: 131,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.060274728599154e-10\n",
            "Epoch: 132,AUC: 0.4749, AP: 0.4824\n",
            "loss: 1.970697632858176e-10\n",
            "Epoch: 133,AUC: 0.4749, AP: 0.4821\n",
            "loss: 3.4039321361589714e-10\n",
            "Epoch: 134,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.5081606236376786e-10\n",
            "Epoch: 135,AUC: 0.4749, AP: 0.4820\n",
            "loss: 2.1498519631180102e-10\n",
            "Epoch: 136,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.6873148151196347e-10\n",
            "Epoch: 137,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.970697632858176e-10\n",
            "Epoch: 138,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.6123889723385076e-10\n",
            "Epoch: 139,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.6873148151196347e-10\n",
            "Epoch: 140,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.1645032160778612e-10\n",
            "Epoch: 141,AUC: 0.4749, AP: 0.4821\n",
            "loss: 2.1498519631180102e-10\n",
            "Epoch: 142,AUC: 0.4749, AP: 0.4825\n",
            "loss: -4.4788582564958546e-11\n",
            "Epoch: 143,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.7915433025983418e-10\n",
            "Epoch: 144,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.5228118765975296e-10\n",
            "Epoch: 145,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.6123889723385076e-10\n",
            "Epoch: 146,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.3436574075598173e-10\n",
            "Epoch: 147,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.3436574075598173e-10\n",
            "Epoch: 148,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.4935093706778275e-10\n",
            "Epoch: 149,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.970697632858176e-10\n",
            "Epoch: 150,AUC: 0.4749, AP: 0.4820\n",
            "loss: 1.2540803118188393e-10\n",
            "Epoch: 151,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.060274728599154e-10\n",
            "Epoch: 152,AUC: 0.4749, AP: 0.4821\n",
            "loss: 9.85348816429088e-11\n",
            "Epoch: 153,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.5830866051966836e-11\n",
            "Epoch: 154,AUC: 0.4749, AP: 0.4825\n",
            "loss: -1.7915433025983418e-11\n",
            "Epoch: 155,AUC: 0.4749, AP: 0.4825\n",
            "loss: 6.270401559094196e-11\n",
            "Epoch: 156,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.2540803118188393e-10\n",
            "Epoch: 157,AUC: 0.4749, AP: 0.4825\n",
            "loss: -8.957716512991709e-12\n",
            "Epoch: 158,AUC: 0.4749, AP: 0.4825\n",
            "loss: 8.957716512991709e-11\n",
            "Epoch: 159,AUC: 0.4749, AP: 0.4822\n",
            "loss: 3.5830866051966836e-11\n",
            "Epoch: 160,AUC: 0.4749, AP: 0.4820\n",
            "loss: 2.1498519631180102e-10\n",
            "Epoch: 161,AUC: 0.4749, AP: 0.4820\n",
            "loss: -7.166173210393367e-11\n",
            "Epoch: 162,AUC: 0.4749, AP: 0.4825\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 163,AUC: 0.4749, AP: 0.4820\n",
            "loss: -1.5228118765975296e-10\n",
            "Epoch: 164,AUC: 0.4749, AP: 0.4821\n",
            "loss: 8.957716512991709e-11\n",
            "Epoch: 165,AUC: 0.4749, AP: 0.4821\n",
            "loss: -1.0749259815590051e-10\n",
            "Epoch: 166,AUC: 0.4749, AP: 0.4820\n",
            "loss: -8.957716512991709e-12\n",
            "Epoch: 167,AUC: 0.4749, AP: 0.4822\n",
            "loss: -0.0\n",
            "Epoch: 168,AUC: 0.4749, AP: 0.4822\n",
            "loss: 8.061944861692538e-11\n",
            "Epoch: 169,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.5228118765975296e-10\n",
            "Epoch: 170,AUC: 0.4749, AP: 0.4822\n",
            "loss: -2.6873149538975127e-11\n",
            "Epoch: 171,AUC: 0.4749, AP: 0.4821\n",
            "loss: 8.957716512991709e-12\n",
            "Epoch: 172,AUC: 0.4749, AP: 0.4821\n",
            "loss: -1.6123889723385076e-10\n",
            "Epoch: 173,AUC: 0.4749, AP: 0.4825\n",
            "loss: -1.3436574075598173e-10\n",
            "Epoch: 174,AUC: 0.4749, AP: 0.4825\n",
            "loss: 6.270401559094196e-11\n",
            "Epoch: 175,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.060274728599154e-10\n",
            "Epoch: 176,AUC: 0.4749, AP: 0.4822\n",
            "loss: -7.166173210393367e-11\n",
            "Epoch: 177,AUC: 0.4749, AP: 0.4821\n",
            "loss: -2.6873149538975127e-11\n",
            "Epoch: 178,AUC: 0.4749, AP: 0.4822\n",
            "loss: -1.3436574075598173e-10\n",
            "Epoch: 179,AUC: 0.4749, AP: 0.4825\n",
            "loss: -1.7019660680794857e-10\n",
            "Epoch: 180,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.3436574075598173e-10\n",
            "Epoch: 181,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.970697632858176e-10\n",
            "Epoch: 182,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.5977378581565347e-10\n",
            "Epoch: 183,AUC: 0.4749, AP: 0.4825\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 184,AUC: 0.4749, AP: 0.4825\n",
            "loss: -3.5830866051966836e-11\n",
            "Epoch: 185,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.4332346420786735e-10\n",
            "Epoch: 186,AUC: 0.4749, AP: 0.4821\n",
            "loss: 8.957716512991709e-11\n",
            "Epoch: 187,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 188,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.6123889723385076e-10\n",
            "Epoch: 189,AUC: 0.4749, AP: 0.4825\n",
            "loss: 3.045623753195059e-10\n",
            "Epoch: 190,AUC: 0.4749, AP: 0.4821\n",
            "loss: 4.4788582564958546e-11\n",
            "Epoch: 191,AUC: 0.4749, AP: 0.4820\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 192,AUC: 0.4749, AP: 0.4821\n",
            "loss: 2.6873149538975127e-11\n",
            "Epoch: 193,AUC: 0.4749, AP: 0.4822\n",
            "loss: 7.166173210393367e-11\n",
            "Epoch: 194,AUC: 0.4749, AP: 0.4820\n",
            "loss: 8.957716512991709e-11\n",
            "Epoch: 195,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.3436574075598173e-10\n",
            "Epoch: 196,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.2540803118188393e-10\n",
            "Epoch: 197,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.2394291976368663e-10\n",
            "Epoch: 198,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.0749259815590051e-10\n",
            "Epoch: 199,AUC: 0.4749, AP: 0.4822\n",
            "loss: 8.957716512991709e-12\n",
            "Epoch: 200,AUC: 0.4749, AP: 0.4821\n",
            "loss: -0.0\n",
            "Epoch: 201,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.6123889723385076e-10\n",
            "Epoch: 202,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.0749259815590051e-10\n",
            "Epoch: 203,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.5228118765975296e-10\n",
            "Epoch: 204,AUC: 0.4749, AP: 0.4821\n",
            "loss: 4.4788582564958546e-11\n",
            "Epoch: 205,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.0749259815590051e-10\n",
            "Epoch: 206,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.1645032160778612e-10\n",
            "Epoch: 207,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.6873149538975127e-11\n",
            "Epoch: 208,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.7915433025983418e-11\n",
            "Epoch: 209,AUC: 0.4749, AP: 0.4821\n",
            "loss: -1.2540803118188393e-10\n",
            "Epoch: 210,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.6873149538975127e-11\n",
            "Epoch: 211,AUC: 0.4749, AP: 0.4824\n",
            "loss: -2.5977378581565347e-10\n",
            "Epoch: 212,AUC: 0.4749, AP: 0.4824\n",
            "loss: 1.1645032160778612e-10\n",
            "Epoch: 213,AUC: 0.4749, AP: 0.4822\n",
            "loss: -5.3746299077950255e-11\n",
            "Epoch: 214,AUC: 0.4749, AP: 0.4819\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 215,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.3290064321557225e-10\n",
            "Epoch: 216,AUC: 0.4749, AP: 0.4825\n",
            "loss: 8.061944861692538e-11\n",
            "Epoch: 217,AUC: 0.4749, AP: 0.4822\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 218,AUC: 0.4749, AP: 0.4821\n",
            "loss: -8.957716512991709e-11\n",
            "Epoch: 219,AUC: 0.4749, AP: 0.4825\n",
            "loss: -4.4788582564958546e-11\n",
            "Epoch: 220,AUC: 0.4749, AP: 0.4825\n",
            "loss: -4.4788582564958546e-11\n",
            "Epoch: 221,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.3436574075598173e-10\n",
            "Epoch: 222,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.6123889723385076e-10\n",
            "Epoch: 223,AUC: 0.4749, AP: 0.4822\n",
            "loss: -0.0\n",
            "Epoch: 224,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.4185833891188224e-10\n",
            "Epoch: 225,AUC: 0.4749, AP: 0.4820\n",
            "loss: -5.3746299077950255e-11\n",
            "Epoch: 226,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.6873149538975127e-11\n",
            "Epoch: 227,AUC: 0.4749, AP: 0.4821\n",
            "loss: -1.5228118765975296e-10\n",
            "Epoch: 228,AUC: 0.4749, AP: 0.4822\n",
            "loss: 2.6873149538975127e-11\n",
            "Epoch: 229,AUC: 0.4749, AP: 0.4822\n",
            "loss: -5.3746299077950255e-11\n",
            "Epoch: 230,AUC: 0.4749, AP: 0.4825\n",
            "loss: 2.5977378581565347e-10\n",
            "Epoch: 231,AUC: 0.4749, AP: 0.4821\n",
            "loss: 2.1498519631180102e-10\n",
            "Epoch: 232,AUC: 0.4749, AP: 0.4822\n",
            "loss: 9.85348816429088e-11\n",
            "Epoch: 233,AUC: 0.4749, AP: 0.4821\n",
            "loss: 8.957716512991709e-12\n",
            "Epoch: 234,AUC: 0.4749, AP: 0.4822\n",
            "loss: -4.4788582564958546e-11\n",
            "Epoch: 235,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.7019660680794857e-10\n",
            "Epoch: 236,AUC: 0.4749, AP: 0.4822\n",
            "loss: -1.7915433025983418e-11\n",
            "Epoch: 237,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.7915433025983418e-11\n",
            "Epoch: 238,AUC: 0.4749, AP: 0.4823\n",
            "loss: 9.85348816429088e-11\n",
            "Epoch: 239,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.1645032160778612e-10\n",
            "Epoch: 240,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 241,AUC: 0.4749, AP: 0.4819\n",
            "loss: 3.5830866051966836e-11\n",
            "Epoch: 242,AUC: 0.4749, AP: 0.4822\n",
            "loss: 7.166173210393367e-11\n",
            "Epoch: 243,AUC: 0.4749, AP: 0.4821\n",
            "loss: 2.6873149538975127e-11\n",
            "Epoch: 244,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.6123889723385076e-10\n",
            "Epoch: 245,AUC: 0.4749, AP: 0.4822\n",
            "loss: -6.270401559094196e-11\n",
            "Epoch: 246,AUC: 0.4749, AP: 0.4822\n",
            "loss: -5.3746299077950255e-11\n",
            "Epoch: 247,AUC: 0.4749, AP: 0.4822\n",
            "loss: -1.1645032160778612e-10\n",
            "Epoch: 248,AUC: 0.4749, AP: 0.4821\n",
            "loss: -0.0\n",
            "Epoch: 249,AUC: 0.4749, AP: 0.4825\n",
            "loss: -1.0749259815590051e-10\n",
            "Epoch: 250,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.6123889723385076e-10\n",
            "Epoch: 251,AUC: 0.4749, AP: 0.4822\n",
            "loss: 3.5830866051966836e-11\n",
            "Epoch: 252,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.5228118765975296e-10\n",
            "Epoch: 253,AUC: 0.4749, AP: 0.4821\n",
            "loss: -6.270401559094196e-11\n",
            "Epoch: 254,AUC: 0.4749, AP: 0.4822\n",
            "loss: -4.4788582564958546e-11\n",
            "Epoch: 255,AUC: 0.4749, AP: 0.4821\n",
            "loss: -2.866469284157347e-10\n",
            "Epoch: 256,AUC: 0.4749, AP: 0.4825\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 257,AUC: 0.4749, AP: 0.4821\n",
            "loss: 8.061944861692538e-11\n",
            "Epoch: 258,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.7915433025983418e-10\n",
            "Epoch: 259,AUC: 0.4749, AP: 0.4825\n",
            "loss: -8.957716512991709e-11\n",
            "Epoch: 260,AUC: 0.4749, AP: 0.4821\n",
            "loss: 6.270401559094196e-11\n",
            "Epoch: 261,AUC: 0.4749, AP: 0.4821\n",
            "loss: -7.166173210393367e-11\n",
            "Epoch: 262,AUC: 0.4749, AP: 0.4824\n",
            "loss: -5.3746299077950255e-11\n",
            "Epoch: 263,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.970697632858176e-10\n",
            "Epoch: 264,AUC: 0.4749, AP: 0.4821\n",
            "loss: -1.6123889723385076e-10\n",
            "Epoch: 265,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.881120537117198e-10\n",
            "Epoch: 266,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.1645032160778612e-10\n",
            "Epoch: 267,AUC: 0.4749, AP: 0.4822\n",
            "loss: 1.7915433025983418e-11\n",
            "Epoch: 268,AUC: 0.4749, AP: 0.4826\n",
            "loss: -7.166173210393367e-11\n",
            "Epoch: 269,AUC: 0.4749, AP: 0.4822\n",
            "loss: 6.270401559094196e-11\n",
            "Epoch: 270,AUC: 0.4749, AP: 0.4820\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 271,AUC: 0.4749, AP: 0.4821\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 272,AUC: 0.4749, AP: 0.4825\n",
            "loss: 8.957716512991709e-11\n",
            "Epoch: 273,AUC: 0.4749, AP: 0.4823\n",
            "loss: -8.957716512991709e-12\n",
            "Epoch: 274,AUC: 0.4749, AP: 0.4825\n",
            "loss: -1.7915433025983418e-11\n",
            "Epoch: 275,AUC: 0.4749, AP: 0.4822\n",
            "loss: 7.166173210393367e-11\n",
            "Epoch: 276,AUC: 0.4749, AP: 0.4824\n",
            "loss: -8.061944861692538e-11\n",
            "Epoch: 277,AUC: 0.4749, AP: 0.4825\n",
            "loss: 1.7915433025983418e-10\n",
            "Epoch: 278,AUC: 0.4749, AP: 0.4821\n",
            "loss: -1.7915433025983418e-10\n",
            "Epoch: 279,AUC: 0.4749, AP: 0.4822\n",
            "loss: -7.166173210393367e-11\n",
            "Epoch: 280,AUC: 0.4749, AP: 0.4825\n",
            "loss: -9.85348816429088e-11\n",
            "Epoch: 281,AUC: 0.4749, AP: 0.4820\n",
            "loss: -8.957716512991709e-12\n",
            "Epoch: 282,AUC: 0.4749, AP: 0.4822\n",
            "loss: -1.1645032160778612e-10\n",
            "Epoch: 283,AUC: 0.4749, AP: 0.4822\n",
            "loss: -2.6873149538975127e-11\n",
            "Epoch: 284,AUC: 0.4749, AP: 0.4825\n",
            "loss: 6.270401559094196e-11\n",
            "Epoch: 285,AUC: 0.4749, AP: 0.4824\n",
            "loss: 1.1645032160778612e-10\n",
            "Epoch: 286,AUC: 0.4749, AP: 0.4822\n",
            "loss: -8.061944861692538e-11\n",
            "Epoch: 287,AUC: 0.4749, AP: 0.4822\n",
            "loss: 8.061944861692538e-11\n",
            "Epoch: 288,AUC: 0.4749, AP: 0.4825\n",
            "loss: -3.5830866051966836e-11\n",
            "Epoch: 289,AUC: 0.4749, AP: 0.4822\n",
            "loss: -6.270401559094196e-11\n",
            "Epoch: 290,AUC: 0.4749, AP: 0.4822\n",
            "loss: -8.957716512991709e-12\n",
            "Epoch: 291,AUC: 0.4749, AP: 0.4821\n",
            "loss: 5.3746299077950255e-11\n",
            "Epoch: 292,AUC: 0.4749, AP: 0.4822\n",
            "loss: -1.3436574075598173e-10\n",
            "Epoch: 293,AUC: 0.4749, AP: 0.4821\n",
            "loss: -0.0\n",
            "Epoch: 294,AUC: 0.4749, AP: 0.4825\n",
            "loss: 8.957716512991709e-12\n",
            "Epoch: 295,AUC: 0.4749, AP: 0.4822\n",
            "loss: -0.0\n",
            "Epoch: 296,AUC: 0.4749, AP: 0.4821\n",
            "loss: 1.2540803118188393e-10\n",
            "Epoch: 297,AUC: 0.4749, AP: 0.4825\n",
            "loss: -3.5830866051966836e-11\n",
            "Epoch: 298,AUC: 0.4749, AP: 0.4822\n",
            "loss: 8.957716512991709e-11\n",
            "Epoch: 299,AUC: 0.4749, AP: 0.4821\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, epochs):\n",
        "    loss = train()\n",
        "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
        "    print('Epoch: {:03d},AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
        "\n",
        "    #writer.add_scalar('loss train',loss,epoch)\n",
        "    #writer.add_scalar('auc train',auc,epoch) # new line\n",
        "    #writer.add_scalar('ap train',ap,epoch)   # new line"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wLulKqenhpys"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Graph VAE-RNN decoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}